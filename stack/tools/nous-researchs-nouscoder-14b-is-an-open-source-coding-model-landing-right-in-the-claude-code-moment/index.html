<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><script type="module">(window.top!==window.self||window.location.hostname==="agentsicworld.pages.dev")&&(window.top.location.href="https://www.agentsicworld.com");</script><meta http-equiv="Content-Security-Policy" content="default-src 'self';
script-src 'self' https://www.google-analytics.com;
style-src 'self' https://fonts.googleapis.com;
img-src 'self' data: https://www.google-analytics.com https://www.googletagmanager.com https://i.pravatar.cc;
connect-src 'self' https://*.supabase.co https://www.google-analytics.com https://stats.g.doubleclick.net;
font-src 'self' https://fonts.gstatic.com; frame-ancestors 'none';"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.ico"><meta name="generator" content="Astro v5.16.10"><title>Nous Research&#39;s NousCoder-14B is an open-source coding model landing right in the Claude Code moment | Agentic Stack | Curated Research, News &amp; Implementation Stack.</title><meta name="description" content="Curated Research, News &#38; Implementation Stack."><meta property="og:title" content="Nous Research's NousCoder-14B is an open-source coding model landing right in the Claude Code moment | Agentic Stack | Curated Research, News &#38; Implementation Stack."><meta property="og:description" content="Curated Research, News &#38; Implementation Stack."><meta property="og:type" content="website"><meta name="google-site-verification" content="TudBB1tWAlqMcSVbAgr2cOJBSwGoLAwsrt3FEe0w7Ds"><link rel="stylesheet" href="/assets/main.css"><link rel="stylesheet" href="/_astro/about.C7jC7f-A.css"></head> <body>  <main class="tool-page"> <div class="tool-header"> <a href="/stack" class="back-link">← Back to Agentic Stack</a> <div class="meta-row"> <span class="category-tag">case-study</span> <span class="layer-tag">brain</span> </div> <h1>Nous Research&#39;s NousCoder-14B is an open-source coding model landing right in the Claude Code moment</h1> </div> <div class="tool-grid"> <section class="main-content"> <h2>Executive Summary</h2> <div class="prose"> &lt;p&gt;&lt;a href=&quot;https://nousresearch.com/&quot;&gt;Nous Research&lt;/a&gt;, the open-source artificial intelligence startup backed by crypto venture firm &lt;a href=&quot;https://www.paradigm.xyz/&quot;&gt;Paradigm&lt;/a&gt;, released a new competitive programming model on Monday that it says matches or exceeds several larger proprietary systems — trained in just four days using 48 of Nvidia&amp;#x27;s latest &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/dgx-b200/&quot;&gt;B200 graphics processors&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The model, called &lt;a href=&quot;https://huggingface.co/NousResearch/NousCoder-14B&quot;&gt;NousCoder-14B&lt;/a&gt;, is another entry in a crowded field of AI coding assistants, but arrives at a particularly charged moment: &lt;a href=&quot;https://claude.com/product/claude-code&quot;&gt;Claude Code&lt;/a&gt;, the agentic programming tool from rival Anthropic, has dominated social media discussion since New Year&amp;#x27;s Day, with developers posting &lt;a href=&quot;https://x.com/0xDesigner/status/2008202211738648767?s=20&quot;&gt;breathless&lt;/a&gt; &lt;a href=&quot;https://x.com/hayesdev_/status/2008043379805048948&quot;&gt;testimonials&lt;/a&gt; &lt;a href=&quot;https://x.com/0xDesigner/status/2008202211738648767?s=20&quot;&gt;about its capabilities&lt;/a&gt;. The simultaneous developments underscore how quickly AI-assisted software development is evolving — and how fiercely companies large and small are competing to capture what many believe will become a foundational technology for how software gets written.&lt;/p&gt;&lt;p&gt;&lt;span&gt;type: &lt;!-- --&gt;embedded-entry-inline&lt;!-- --&gt; id: &lt;!-- --&gt;74cSyrq6OUrp9SEQ5zOUSl&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://nousresearch.com/nouscoder-14b-a-competitive-olympiad-programming-model/&quot;&gt;NousCoder-14B&lt;/a&gt; achieves a 67.87 percent accuracy rate on &lt;a href=&quot;https://livecodebench.github.io/&quot;&gt;LiveCodeBench v6&lt;/a&gt;, a standardized evaluation that tests models on competitive programming problems published between August 2024 and May 2025. That figure represents a 7.08 percentage point improvement over the base model it was trained from, Alibaba&amp;#x27;s &lt;a href=&quot;https://huggingface.co/Qwen/Qwen3-14B&quot;&gt;Qwen3-14B&lt;/a&gt;, according to Nous Research&amp;#x27;s technical report published alongside the release.&lt;/p&gt;&lt;p&gt;&amp;quot;I gave Claude Code a description of the problem, it generated what we built last year in an hour,&amp;quot; &lt;a href=&quot;https://www.reddit.com/r/OpenAI/comments/1q2uuil/google_engineer_im_not_joking_and_this_isnt_funny/&quot;&gt;wrote Jaana Dogan&lt;/a&gt;, a principal engineer at Google responsible for the Gemini API, in a viral post on X last week that captured the prevailing mood around AI coding tools. Dogan was describing a distributed agent orchestration system her team had spent a year developing — a system Claude Code approximated from a three-paragraph prompt.&lt;/p&gt;&lt;p&gt;The juxtaposition is instructive: while Anthropic&amp;#x27;s &lt;a href=&quot;https://venturebeat.com/technology/the-creator-of-claude-code-just-revealed-his-workflow-and-developers-are&quot;&gt;Claude Code has captured imaginations&lt;/a&gt; with demonstrations of end-to-end software development, Nous Research is betting that open-source alternatives trained on verifiable problems can close the gap — and that transparency in how these models are built matters as much as raw capability.&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt;&lt;b&gt;How Nous Research built an AI coding model that anyone can replicate&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;What distinguishes the &lt;a href=&quot;https://huggingface.co/NousResearch/NousCoder-14B&quot;&gt;NousCoder-14B&lt;/a&gt; release from many competitor announcements is its radical openness. Nous Research published not just the &lt;a href=&quot;https://huggingface.co/NousResearch/NousCoder-14B&quot;&gt;model weights&lt;/a&gt; but the &lt;a href=&quot;https://github.com/NousResearch/atropos/pull/296&quot;&gt;complete reinforcement learning environment&lt;/a&gt;, benchmark suite, and training harness — built on the company&amp;#x27;s &lt;a href=&quot;https://github.com/NousResearch/atropos/pull/296&quot;&gt;Atropos framework &lt;/a&gt;— enabling any researcher with sufficient compute to &lt;a href=&quot;https://wandb.ai/jli505/qwen14b/reports/HermesCoder-14B--VmlldzoxNTQ5Nzc0MQ?accessToken=4pt3stwyh4x83zqe2jgoo5j9b7j07jbe5omf2n40lray3tih17vfkavjootvnw8o&quot;&gt;reproduce or extend the work&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&amp;quot;Open-sourcing the Atropos stack provides the necessary infrastructure for reproducible olympiad-level reasoning research,&amp;quot; &lt;a href=&quot;https://x.com/o_mega___/status/2008907268700475450?s=20&quot;&gt;noted one observer on X&lt;/a&gt;, summarizing the significance for the academic and open-source communities.&lt;/p&gt;&lt;p&gt;The model was trained by &lt;a href=&quot;https://x.com/JoeLi5050&quot;&gt;Joe Li&lt;/a&gt;, a researcher in residence at Nous Research and a former competitive programmer himself. Li&amp;#x27;s &lt;a href=&quot;https://nousresearch.com/nouscoder-14b-a-competitive-olympiad-programming-model/&quot;&gt;technical report &lt;/a&gt;reveals an unexpectedly personal dimension: he compared the model&amp;#x27;s improvement trajectory to his own journey on Codeforces, the competitive programming platform where participants earn ratings based on contest performance.&lt;/p&gt;&lt;p&gt;Based on rough estimates mapping LiveCodeBench scores to Codeforces ratings, Li calculated that NousCoder-14B&amp;#x27;s improvemen t— from approximately the 1600-1750 rating range to 2100-2200 — mirrors a leap that took him nearly two years of sustained practice between ages 14 and 16. The model accomplished the equivalent in four days.&lt;/p&gt;&lt;p&gt;&amp;quot;Watching that final training run unfold was quite a surreal experience,&amp;quot; Li wrote in the technical report.&lt;/p&gt;&lt;p&gt;But Li was quick to note an important caveat that speaks to broader questions about AI efficiency: he solved roughly 1,000 problems during those two years, while the model required 24,000. Humans, at least for now, remain dramatically more sample-efficient learners.&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt;&lt;b&gt;Inside the reinforcement learning system that trains on 24,000 competitive programming problems&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://huggingface.co/NousResearch/NousCoder-14B&quot;&gt;NousCoder-14B&lt;/a&gt;&amp;#x27;s training process offers a window into the increasingly sophisticated techniques researchers use to improve AI reasoning capabilities through reinforcement learning.&lt;/p&gt;&lt;p&gt;The approach relies on what researchers call &amp;quot;verifiable rewards&amp;quot; — a system where the model generates code solutions, those solutions are executed against test cases, and the model receives a simple binary signal: correct or incorrect. This feedback loop, while conceptually straightforward, requires significant infrastructure to execute at scale.&lt;/p&gt;&lt;p&gt;Nous Research used &lt;a href=&quot;https://modal.com/&quot;&gt;Modal&lt;/a&gt;, a cloud computing platform, to run sandboxed code execution in parallel. Each of the 24,000 training problems contains hundreds of test cases on average, and the system must verify that generated code produces correct outputs within time and memory constraints — 15 seconds and 4 gigabytes, respectively.&lt;/p&gt;&lt;p&gt;The training employed a technique called &lt;a href=&quot;https://dapo-sia.github.io/&quot;&gt;DAPO (Dynamic Sampling Policy Optimization)&lt;/a&gt;, which the researchers found performed slightly better than alternatives in their experiments. A key innovation involves &amp;quot;dynamic sampling&amp;quot; — discarding training examples where the model either solves all attempts or fails all attempts, since these provide no useful gradient signal for learning.&lt;/p&gt;&lt;p&gt;The researchers also adopted &amp;quot;iterative context extension,&amp;quot; first training the model with a 32,000-token context window before expanding to 40,000 tokens. During evaluation, extending the context further to approximately 80,000 tokens produced the best results, with accuracy reaching 67.87 percent.&lt;/p&gt;&lt;p&gt;Perhaps most significantly, the training pipeline overlaps inference and verification — as soon as the model generates a solution, it begins work on the next problem while the previous solution is being checked. This pipelining, combined with asynchronous training where multiple model instances work in parallel, maximizes hardware utilization on expensive GPU clusters.&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt;&lt;b&gt;The looming data shortage that could slow AI coding model progress&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Buried in Li&amp;#x27;s &lt;a href=&quot;https://nousresearch.com/nouscoder-14b-a-competitive-olympiad-programming-model/&quot;&gt;technical report&lt;/a&gt; is a finding with significant implications for the future of AI development: the training dataset for NousCoder-14B encompasses &amp;quot;a significant portion of all readily available, verifiable competitive programming problems in a standardized dataset format.&amp;quot;&lt;/p&gt;&lt;p&gt;In other words, for this particular domain, the researchers are approaching the limits of high-quality training data.&lt;/p&gt;&lt;p&gt;&amp;quot;The total number of competitive programming problems on the Internet is roughly the same order of magnitude,&amp;quot; Li wrote, referring to the 24,000 problems used for training. &amp;quot;This suggests that within the competitive programming domain, we have approached the limits of high-quality data.&amp;quot;&lt;/p&gt;&lt;p&gt;This observation echoes growing concern across the AI industry about data constraints. While compute continues to scale according to well-understood economic and engineering principles, training data is &amp;quot;increasingly finite,&amp;quot; as Li put it.&lt;/p&gt;&lt;p&gt;&amp;quot;It appears that some of the most important research that needs to be done in the future will be in the areas of synthetic data generation and data efficient algorithms and architectures,&amp;quot; he concluded.&lt;/p&gt;&lt;p&gt;The challenge is particularly acute for competitive programming because the domain requires problems with known correct solutions that can be verified automatically. Unlike natural language tasks where human evaluation or proxy metrics suffice, code either works or it doesn&amp;#x27;t — making synthetic data generation considerably more difficult.&lt;/p&gt;&lt;p&gt;Li identified one potential avenue: training models not just to solve problems but to generate solvable problems, enabling a form of self-play similar to techniques that proved successful in game-playing AI systems. &amp;quot;Once synthetic problem generation is solved, self-play becomes a very interesting direction,&amp;quot; he wrote.&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt;&lt;b&gt;A $65 million bet that open-source AI can compete with Big Tech&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Nous Research has carved out a distinctive position in the AI landscape: a company committed to &lt;a href=&quot;https://nousresearch.com/&quot;&gt;open-source releases&lt;/a&gt; that compete with — and sometimes exceed — proprietary alternatives.&lt;/p&gt;&lt;p&gt;The company raised&lt;a href=&quot;https://fortune.com/crypto/2025/04/25/paradigm-nous-research-crypto-ai-venture-capital-deepseek-openai-blockchain/&quot;&gt; $50 million in April 2025&lt;/a&gt; in a round led by Paradigm, the cryptocurrency-focused venture firm founded by Coinbase co-founder Fred Ehrsam. Total funding reached $65 million, according to some reports. The investment reflected growing interest in decentralized approaches to AI training, an area where Nous Research has developed its &lt;a href=&quot;https://psyche.network/&quot;&gt;Psyche platform&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Previous releases include &lt;a href=&quot;https://hermes4.nousresearch.com/&quot;&gt;Hermes 4&lt;/a&gt;, a family of models that we reported &amp;quot;&lt;a href=&quot;https://venturebeat.com/ai/nous-research-drops-hermes-4-ai-models-that-outperform-chatgpt-without-content-restrictions&quot;&gt;outperform ChatGPT without content restrictions&lt;/a&gt;,&amp;quot; and DeepHermes-3, which the company described as the first &amp;quot;&lt;a href=&quot;https://venturebeat.com/ai/personalized-unrestricted-ai-lab-nous-research-launches-first-toggle-on-reasoning-model-deephermes-3&quot;&gt;toggle-on reasoning model&lt;/a&gt;&amp;quot; — allowing users to activate extended thinking capabilities on demand.&lt;/p&gt;&lt;p&gt;The company has cultivated a distinctive aesthetic and community, prompting some skepticism about whether style might overshadow substance. &amp;quot;Ofc i&amp;#x27;m gonna believe an anime pfp company. stop benchmarkmaxxing ffs,&amp;quot; &lt;a href=&quot;https://x.com/shydev69/status/2008654826356535510?s=20&quot;&gt;wrote one critic on X&lt;/a&gt;, referring to Nous Research&amp;#x27;s anime-style branding and the industry practice of optimizing for benchmark performance.&lt;/p&gt;&lt;p&gt;Others raised technical questions. &amp;quot;&lt;a href=&quot;https://x.com/yehor_smoliakov/status/2008659681489940757?s=20&quot;&gt;Based on the benchmark, Nemotron is better&lt;/a&gt;,&amp;quot; noted one commenter, referring to Nvidia&amp;#x27;s family of language models. Another asked whether &lt;a href=&quot;https://huggingface.co/NousResearch/NousCoder-14B&quot;&gt;NousCoder-14B&lt;/a&gt; is &amp;quot;agentic focused or just &amp;#x27;one shot&amp;#x27; coding&amp;quot; — a distinction that matters for practical software development, where iterating on feedback typically produces better results than single attempts.&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt;&lt;b&gt;What researchers say must happen next for AI coding tools to keep improving&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The release includes several directions for future work that hint at where AI coding research may be heading.&lt;/p&gt;&lt;p&gt;Multi-turn reinforcement learning tops the list. Currently, the model receives only a final binary reward — pass or fail — after generating a solution. But competitive programming problems typically include public test cases that provide intermediate feedback: compilation errors, incorrect outputs, time limit violations. Training models to incorporate this feedback across multiple attempts could significantly improve performance.&lt;/p&gt;&lt;p&gt;Controlling response length also remains a challenge. The researchers found that incorrect solutions tended to be longer than correct ones, and response lengths quickly saturated available context windows during training — a pattern that various algorithmic modifications failed to resolve.&lt;/p&gt;&lt;p&gt;Perhaps most ambitiously, Li proposed &amp;quot;problem generation and self-play&amp;quot; — training models to both solve and create programming problems. This would address the data scarcity problem directly by enabling models to generate their own training curricula.&lt;/p&gt;&lt;p&gt;&amp;quot;Humans are great at generating interesting and useful problems for other competitive programmers, but it appears that there still exists a significant gap in LLM capabilities in creative problem generation,&amp;quot; Li wrote.&lt;/p&gt;&lt;p&gt;The model is &lt;a href=&quot;https://huggingface.co/NousResearch/NousCoder-14B&quot;&gt;available now on Hugging Face&lt;/a&gt; under an Apache 2.0 license. For researchers and developers who want to build on the work, Nous Research has published the complete &lt;a href=&quot;https://github.com/NousResearch/atropos/pull/296&quot;&gt;Atropos training stack&lt;/a&gt; alongside it.&lt;/p&gt;&lt;p&gt;What took Li two years of adolescent dedication to achieve—climbing from a 1600-level novice to a 2100-rated competitor on Codeforces—an AI replicated in 96 hours. He needed 1,000 problems. The model needed 24,000. But soon enough, these systems may learn to write their own problems, teach themselves, and leave human benchmarks behind entirely.&lt;/p&gt;&lt;p&gt;The question is no longer whether machines can learn to code. It&amp;#x27;s whether they&amp;#x27;ll soon be better teachers than we ever were.&lt;/p&gt;&lt;p&gt;
&lt;/p&gt; </div> <div class="snippet-section"> <h3>Original Context</h3> <p>&lt;p&gt;&lt;a href=&quot;https://nousresearch.com/&quot;&gt;Nous Research&lt;/a&gt;, the open-source artificial intelligence startup backed by crypto venture firm &lt;a href=&quot;https://www.paradigm.xyz/&quot;&gt;Paradigm&lt;/a&gt;, released a new competitive programming model on Monday that it says matches or exceeds several larger proprietary systems — trained in just four days using 48 of Nvidia&amp;#x27;s latest &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/dgx-b200/&quot;&gt;B200 graphics processors&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The model, called &lt;a href=&quot;https://huggingface.co/NousResearch/NousCoder-14B&quot;&gt;NousCoder-14B&lt;/a&gt;, is another entry in a crowded field of AI coding assistants, but arrives at a particularly charged moment: &lt;a href=&quot;https://claude.com/product/claude-code&quot;&gt;Claude Code&lt;/a&gt;, the agentic programming tool from rival Anthropic, has dominated social media discussion since New Year&amp;#x27;s Day, with developers posting &lt;a href=&quot;https://x.com/0xDesigner/status/2008202211738648767?s=20&quot;&gt;breathless&lt;/a&gt; &lt;a href=&quot;https://x.com/hayesdev_/status/2008043379805048948&quot;&gt;testimonials&lt;/a&gt; &lt;a href=&quot;https://x.com/0xDesigner/status/2008202211738648767?s=20&quot;&gt;about its capabilities&lt;/a&gt;. The simultaneous developments underscore how quickly AI-assisted software development is evolving — and how fiercely companies large and small are competing to capture what many believe will become a foundational technology for how software gets written.&lt;/p&gt;&lt;p&gt;&lt;span&gt;type: &lt;!-- --&gt;embedded-entry-inline&lt;!-- --&gt; id: &lt;!-- --&gt;74cSyrq6OUrp9SEQ5zOUSl&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://nousresearch.com/nouscoder-14b-a-competitive-olympiad-programming-model/&quot;&gt;NousCoder-14B&lt;/a&gt; achieves a 67.87 percent accuracy rate on &lt;a href=&quot;https://livecodebench.github.io/&quot;&gt;LiveCodeBench v6&lt;/a&gt;, a standardized evaluation that tests models on competitive programming problems published between August 2024 and May 2025. That figure represents a 7.08 percentage point improvement over the base model it was trained from, Alibaba&amp;#x27;s &lt;a href=&quot;https://huggingface.co/Qwen/Qwen3-14B&quot;&gt;Qwen3-14B&lt;/a&gt;, according to Nous Research&amp;#x27;s technical report published alongside the release.&lt;/p&gt;&lt;p&gt;&amp;quot;I gave Claude Code a description of the problem, it generated what we built last year in an hour,&amp;quot; &lt;a href=&quot;https://www.reddit.com/r/OpenAI/comments/1q2uuil/google_engineer_im_not_joking_and_this_isnt_funny/&quot;&gt;wrote Jaana Dogan&lt;/a&gt;, a principal engineer at Google responsible for the Gemini API, in a viral post on X last week that captured the prevailing mood around AI coding tools. Dogan was describing a distributed agent orchestration system her team had spent a year developing — a system Claude Code approximated from a three-paragraph prompt.&lt;/p&gt;&lt;p&gt;The juxtaposition is instructive: while Anthropic&amp;#x27;s &lt;a href=&quot;https://venturebeat.com/technology/the-creator-of-claude-code-just-revealed-his-workflow-and-developers-are&quot;&gt;Claude Code has captured imaginations&lt;/a&gt; with demonstrations of end-to-end software development, Nous Research is betting that open-source alternatives trained on verifiable problems can close the gap — and that transparency in how these models are built matters as much as raw capability.&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt;&lt;b&gt;How Nous Research built an AI coding model that anyone can replicate&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;What distinguishes the &lt;a href=&quot;https://huggingface.co/NousResearch/NousCoder-14B&quot;&gt;NousCoder-14B&lt;/a&gt; release from many competitor announcements is its radical openness. Nous Research published not just the &lt;a href=&quot;https://huggingface.co/NousResearch/NousCoder-14B&quot;&gt;model weights&lt;/a&gt; but the &lt;a href=&quot;https://github.com/NousResearch/atropos/pull/296&quot;&gt;complete reinforcement learning environment&lt;/a&gt;, benchmark suite, and training harness — built on the company&amp;#x27;s &lt;a href=&quot;https://github.com/NousResearch/atropos/pull/296&quot;&gt;Atropos framework &lt;/a&gt;— enabling any researcher with sufficient compute to &lt;a href=&quot;https://wandb.ai/jli505/qwen14b/reports/HermesCoder-14B--VmlldzoxNTQ5Nzc0MQ?accessToken=4pt3stwyh4x83zqe2jgoo5j9b7j07jbe5omf2n40lray3tih17vfkavjootvnw8o&quot;&gt;reproduce or extend the work&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&amp;quot;Open-sourcing the Atropos stack provides the necessary infrastructure for reproducible olympiad-level reasoning research,&amp;quot; &lt;a href=&quot;https://x.com/o_mega___/status/2008907268700475450?s=20&quot;&gt;noted one observer on X&lt;/a&gt;, summarizing the significance for the academic and open-source communities.&lt;/p&gt;&lt;p&gt;The model was trained by &lt;a href=&quot;https://x.com/JoeLi5050&quot;&gt;Joe Li&lt;/a&gt;, a researcher in residence at Nous Research and a former competitive programmer himself. Li&amp;#x27;s &lt;a href=&quot;https://nousresearch.com/nouscoder-14b-a-competitive-olympiad-programming-model/&quot;&gt;technical report &lt;/a&gt;reveals an unexpectedly personal dimension: he compared the model&amp;#x27;s improvement trajectory to his own journey on Codeforces, the competitive programming platform where participants earn ratings based on contest performance.&lt;/p&gt;&lt;p&gt;Based on rough estimates mapping LiveCodeBench scores to Codeforces ratings, Li calculated that NousCoder-14B&amp;#x27;s improvemen t— from approximately the 1600-1750 rating range to 2100-2200 — mirrors a leap that took him nearly two years of sustained practice between ages 14 and 16. The model accomplished the equivalent in four days.&lt;/p&gt;&lt;p&gt;&amp;quot;Watching that final training run unfold was quite a surreal experience,&amp;quot; Li wrote in the technical report.&lt;/p&gt;&lt;p&gt;But Li was quick to note an important caveat that speaks to broader questions about AI efficiency: he solved roughly 1,000 problems during those two years, while the model required 24,000. Humans, at least for now, remain dramatically more sample-efficient learners.&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt;&lt;b&gt;Inside the reinforcement learning system that trains on 24,000 competitive programming problems&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://huggingface.co/NousResearch/NousCoder-14B&quot;&gt;NousCoder-14B&lt;/a&gt;&amp;#x27;s training process offers a window into the increasingly sophisticated techniques researchers use to improve AI reasoning capabilities through reinforcement learning.&lt;/p&gt;&lt;p&gt;The approach relies on what researchers call &amp;quot;verifiable rewards&amp;quot; — a system where the model generates code solutions, those solutions are executed against test cases, and the model receives a simple binary signal: correct or incorrect. This feedback loop, while conceptually straightforward, requires significant infrastructure to execute at scale.&lt;/p&gt;&lt;p&gt;Nous Research used &lt;a href=&quot;https://modal.com/&quot;&gt;Modal&lt;/a&gt;, a cloud computing platform, to run sandboxed code execution in parallel. Each of the 24,000 training problems contains hundreds of test cases on average, and the system must verify that generated code produces correct outputs within time and memory constraints — 15 seconds and 4 gigabytes, respectively.&lt;/p&gt;&lt;p&gt;The training employed a technique called &lt;a href=&quot;https://dapo-sia.github.io/&quot;&gt;DAPO (Dynamic Sampling Policy Optimization)&lt;/a&gt;, which the researchers found performed slightly better than alternatives in their experiments. A key innovation involves &amp;quot;dynamic sampling&amp;quot; — discarding training examples where the model either solves all attempts or fails all attempts, since these provide no useful gradient signal for learning.&lt;/p&gt;&lt;p&gt;The researchers also adopted &amp;quot;iterative context extension,&amp;quot; first training the model with a 32,000-token context window before expanding to 40,000 tokens. During evaluation, extending the context further to approximately 80,000 tokens produced the best results, with accuracy reaching 67.87 percent.&lt;/p&gt;&lt;p&gt;Perhaps most significantly, the training pipeline overlaps inference and verification — as soon as the model generates a solution, it begins work on the next problem while the previous solution is being checked. This pipelining, combined with asynchronous training where multiple model instances work in parallel, maximizes hardware utilization on expensive GPU clusters.&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt;&lt;b&gt;The looming data shortage that could slow AI coding model progress&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Buried in Li&amp;#x27;s &lt;a href=&quot;https://nousresearch.com/nouscoder-14b-a-competitive-olympiad-programming-model/&quot;&gt;technical report&lt;/a&gt; is a finding with significant implications for the future of AI development: the training dataset for NousCoder-14B encompasses &amp;quot;a significant portion of all readily available, verifiable competitive programming problems in a standardized dataset format.&amp;quot;&lt;/p&gt;&lt;p&gt;In other words, for this particular domain, the researchers are approaching the limits of high-quality training data.&lt;/p&gt;&lt;p&gt;&amp;quot;The total number of competitive programming problems on the Internet is roughly the same order of magnitude,&amp;quot; Li wrote, referring to the 24,000 problems used for training. &amp;quot;This suggests that within the competitive programming domain, we have approached the limits of high-quality data.&amp;quot;&lt;/p&gt;&lt;p&gt;This observation echoes growing concern across the AI industry about data constraints. While compute continues to scale according to well-understood economic and engineering principles, training data is &amp;quot;increasingly finite,&amp;quot; as Li put it.&lt;/p&gt;&lt;p&gt;&amp;quot;It appears that some of the most important research that needs to be done in the future will be in the areas of synthetic data generation and data efficient algorithms and architectures,&amp;quot; he concluded.&lt;/p&gt;&lt;p&gt;The challenge is particularly acute for competitive programming because the domain requires problems with known correct solutions that can be verified automatically. Unlike natural language tasks where human evaluation or proxy metrics suffice, code either works or it doesn&amp;#x27;t — making synthetic data generation considerably more difficult.&lt;/p&gt;&lt;p&gt;Li identified one potential avenue: training models not just to solve problems but to generate solvable problems, enabling a form of self-play similar to techniques that proved successful in game-playing AI systems. &amp;quot;Once synthetic problem generation is solved, self-play becomes a very interesting direction,&amp;quot; he wrote.&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt;&lt;b&gt;A $65 million bet that open-source AI can compete with Big Tech&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Nous Research has carved out a distinctive position in the AI landscape: a company committed to &lt;a href=&quot;https://nousresearch.com/&quot;&gt;open-source releases&lt;/a&gt; that compete with — and sometimes exceed — proprietary alternatives.&lt;/p&gt;&lt;p&gt;The company raised&lt;a href=&quot;https://fortune.com/crypto/2025/04/25/paradigm-nous-research-crypto-ai-venture-capital-deepseek-openai-blockchain/&quot;&gt; $50 million in April 2025&lt;/a&gt; in a round led by Paradigm, the cryptocurrency-focused venture firm founded by Coinbase co-founder Fred Ehrsam. Total funding reached $65 million, according to some reports. The investment reflected growing interest in decentralized approaches to AI training, an area where Nous Research has developed its &lt;a href=&quot;https://psyche.network/&quot;&gt;Psyche platform&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Previous releases include &lt;a href=&quot;https://hermes4.nousresearch.com/&quot;&gt;Hermes 4&lt;/a&gt;, a family of models that we reported &amp;quot;&lt;a href=&quot;https://venturebeat.com/ai/nous-research-drops-hermes-4-ai-models-that-outperform-chatgpt-without-content-restrictions&quot;&gt;outperform ChatGPT without content restrictions&lt;/a&gt;,&amp;quot; and DeepHermes-3, which the company described as the first &amp;quot;&lt;a href=&quot;https://venturebeat.com/ai/personalized-unrestricted-ai-lab-nous-research-launches-first-toggle-on-reasoning-model-deephermes-3&quot;&gt;toggle-on reasoning model&lt;/a&gt;&amp;quot; — allowing users to activate extended thinking capabilities on demand.&lt;/p&gt;&lt;p&gt;The company has cultivated a distinctive aesthetic and community, prompting some skepticism about whether style might overshadow substance. &amp;quot;Ofc i&amp;#x27;m gonna believe an anime pfp company. stop benchmarkmaxxing ffs,&amp;quot; &lt;a href=&quot;https://x.com/shydev69/status/2008654826356535510?s=20&quot;&gt;wrote one critic on X&lt;/a&gt;, referring to Nous Research&amp;#x27;s anime-style branding and the industry practice of optimizing for benchmark performance.&lt;/p&gt;&lt;p&gt;Others raised technical questions. &amp;quot;&lt;a href=&quot;https://x.com/yehor_smoliakov/status/2008659681489940757?s=20&quot;&gt;Based on the benchmark, Nemotron is better&lt;/a&gt;,&amp;quot; noted one commenter, referring to Nvidia&amp;#x27;s family of language models. Another asked whether &lt;a href=&quot;https://huggingface.co/NousResearch/NousCoder-14B&quot;&gt;NousCoder-14B&lt;/a&gt; is &amp;quot;agentic focused or just &amp;#x27;one shot&amp;#x27; coding&amp;quot; — a distinction that matters for practical software development, where iterating on feedback typically produces better results than single attempts.&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt;&lt;b&gt;What researchers say must happen next for AI coding tools to keep improving&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The release includes several directions for future work that hint at where AI coding research may be heading.&lt;/p&gt;&lt;p&gt;Multi-turn reinforcement learning tops the list. Currently, the model receives only a final binary reward — pass or fail — after generating a solution. But competitive programming problems typically include public test cases that provide intermediate feedback: compilation errors, incorrect outputs, time limit violations. Training models to incorporate this feedback across multiple attempts could significantly improve performance.&lt;/p&gt;&lt;p&gt;Controlling response length also remains a challenge. The researchers found that incorrect solutions tended to be longer than correct ones, and response lengths quickly saturated available context windows during training — a pattern that various algorithmic modifications failed to resolve.&lt;/p&gt;&lt;p&gt;Perhaps most ambitiously, Li proposed &amp;quot;problem generation and self-play&amp;quot; — training models to both solve and create programming problems. This would address the data scarcity problem directly by enabling models to generate their own training curricula.&lt;/p&gt;&lt;p&gt;&amp;quot;Humans are great at generating interesting and useful problems for other competitive programmers, but it appears that there still exists a significant gap in LLM capabilities in creative problem generation,&amp;quot; Li wrote.&lt;/p&gt;&lt;p&gt;The model is &lt;a href=&quot;https://huggingface.co/NousResearch/NousCoder-14B&quot;&gt;available now on Hugging Face&lt;/a&gt; under an Apache 2.0 license. For researchers and developers who want to build on the work, Nous Research has published the complete &lt;a href=&quot;https://github.com/NousResearch/atropos/pull/296&quot;&gt;Atropos training stack&lt;/a&gt; alongside it.&lt;/p&gt;&lt;p&gt;What took Li two years of adolescent dedication to achieve—climbing from a 1600-level novice to a 2100-rated competitor on Codeforces—an AI replicated in 96 hours. He needed 1,000 problems. The model needed 24,000. But soon enough, these systems may learn to write their own problems, teach themselves, and leave human benchmarks behind entirely.&lt;/p&gt;&lt;p&gt;The question is no longer whether machines can learn to code. It&amp;#x27;s whether they&amp;#x27;ll soon be better teachers than we ever were.&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</p> </div> </section> <aside class="sidebar"> <div class="sidebar-card"> <h3>Resource Details</h3> <ul> <li><strong>Category:</strong> Research</li> <li><strong>Layer:</strong> brain</li> </ul> <a href="https://venturebeat.com/technology/nous-researchs-nouscoder-14b-is-an-open-source-coding-model-landing-right-in" target="_blank" class="visit-btn">Visit Original Source</a> </div> </aside> </div> </main>  <div id="supabase-config" data-url="https://nokustzcyvjrzvfblshh.supabase.co" data-key="sb_publishable_WgQZQDyrrfhAkZ-9jZ0Yag_ZE7eHDGy" hidden></div> <!-- ONE global script --> <script type="module" src="/assets/main.js"></script> </body> </html>